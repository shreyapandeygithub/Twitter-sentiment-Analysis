# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis latest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N7JJPUwREBAbBX9iXpWyFOFCmxDW-JrW
"""

!cd /content

from google.colab import files

files.upload()

!mkdir ~/.kaggle

!mv ./kaggle.json ~/.kaggle

!chmod 600 ~/.kaggle/kaggle.json

!pip install kaggle

!kaggle datasets download -d kazanova/sentiment140

!unzip /content/sentiment140.zip

!pip install contractions

import locale
import os
import pandas as pd
import numpy as np
import pickle
from keras.layers import TextVectorization
from keras.layers import Input, Embedding, LSTM, GRU, Dense
from keras.models import Sequential
import tensorflow as tf

from concurrent.futures.thread import ThreadPoolExecutor
import nltk
from multiprocessing import cpu_count
import contractions
import re
from nltk.corpus import stopwords
from nltk import word_tokenize
from spacy.lang.en.stop_words import STOP_WORDS
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

locale.getpreferredencoding = lambda: "UTF-8"

nltk.download("punkt")
nltk.download("stopwords")
nltk.download("averaged_perceptron_tagger")
nltk.download("wordnet")

cpu_count()

data = pd.read_csv("/content/training.1600000.processed.noemoticon.csv",
                   encoding = "latin-1",header = None)

data = data.sample(n = 1000000)

data.shape

data.rename(columns = {0: "target",1: "ids",2: "date",3: "flag",4: "user",5: "text"},
            inplace = True)

data.head()



reduced_data = data.drop(labels = data.columns[1:5],axis=1)

reduced_data.head()

def normalize_tweet(tweet_text):

  return tweet_text.lower()

with ThreadPoolExecutor(max_workers=4) as pool:

  reduced_data["text"] = list(pool.map(normalize_tweet, list(reduced_data["text"])))

reduced_data.head()

def expand_contractions(tweet_text):

  return contractions.fix(tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:

  reduced_data["text"] = list(pool.map(expand_contractions,list(reduced_data["text"])))

reduced_data.head()

regex_pattern = r'@[a-zA-z0-9 ]+|#[a-zA-Z0-9 ]+|\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*|\W+|\d+|<("[^"]*"|\'[^\']*\'|[^\'">])*>|_+|[^\u0000-\u007f]+'

def remove_noisy_tokens(tweet_text):

  return re.sub(pattern=regex_pattern,repl=" ",string=tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:

  reduced_data["text"] = list(pool.map(remove_noisy_tokens,list(reduced_data["text"])))

def remove_remaining_noisy_tokens(tweet_text):

  return re.sub(r'\b\w\b|[^\u0000-\u007f]+|_+|\W+',repl=" ",string=tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:

  reduced_data["text"] = list(pool.map(remove_remaining_noisy_tokens,list(reduced_data["text"])))

def tokenize_tweet_text(tweet_text):

  return word_tokenize(tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:

  reduced_data["text"] = list(pool.map(tokenize_tweet_text,list(reduced_data["text"])))



len(STOP_WORDS)

en_stop_words = list(set(stopwords.words('english')).union(set(STOP_WORDS)))
len(en_stop_words)

def is_stopword(token):

  return not(token in en_stop_words)

def remove_stopwords(tweet_text):

  return list(filter(is_stopword, tweet_text))

"""Here's an example of the result after removing the stop words from a tweet

"""



with ThreadPoolExecutor(max_workers = 4) as pool:

  reduced_data["text"] = list(pool.map(remove_stopwords, list(reduced_data["text"])))

"""In wordnet tags are represented as NN, JJ etc, so to convert them to tags like v for verb, a for adjective etc we convert them using the below function"""

def get_wnet_pos_tag(treebank_tag):
    if treebank_tag[1].startswith('J'):
        return (treebank_tag[0],wn.ADJ)
    elif treebank_tag[1].startswith('V'):
        return (treebank_tag[0],wn.VERB)
    elif treebank_tag[1].startswith('N'):
        return (treebank_tag[0],wn.NOUN)
    elif treebank_tag[1].startswith('R'):
        return (treebank_tag[0],wn.ADV)
    else:
        return (treebank_tag[0],wn.NOUN)

def get_pos_tag(tweet_text):

  return list(map(get_wnet_pos_tag,pos_tag(tweet_text)))

with ThreadPoolExecutor(max_workers=4) as pool:
  reduced_data["text"] = list(pool.map(get_pos_tag, list(reduced_data["text"])))

reduced_data["text"].head()

lemmatizer = WordNetLemmatizer()

def lemmatize_token(token_pos_tuple):

  if token_pos_tuple == None:
    return ""
  else:
    return lemmatizer.lemmatize(word = token_pos_tuple[0], pos = token_pos_tuple[1])

def lemmatize_text(tweet_text):

  if len(tweet_text) > 0:
    return list(map(lemmatize_token, tweet_text))
  else:
    return [""]

with ThreadPoolExecutor(max_workers=4) as pool:

  reduced_data["text"] = list(pool.map(lemmatize_text, list(reduced_data["text"])))

reduced_data["text"].head()

"""Finding max Sequence length"""

max_tokens = 30000
max_sequence_len = max(list(reduced_data["text"].apply(lambda x: len(x))))
print(max_sequence_len)

reduced_rows_idx = np.argwhere(np.array(reduced_data["text"].apply(lambda x: len(x))) >= 10)
print(reduced_rows_idx[1:5])

"""This basically will keep only those rows whose indexes were present above in our data frame"""

reduced_data = reduced_data.iloc[reduced_rows_idx.reshape(reduced_rows_idx.shape[0],)]

"""Here we are combining the tokens of a tweet"""

reduced_data["text"] = list(map(lambda x: " ".join(x), list(reduced_data["text"])))

reduced_data.head()

reduced_data.shape

"""Although there was no change in shape after the following line but its a good practice"""

filtered_reduced_data = reduced_data[reduced_data["text"] != ""]

"""**bold text** Now lets create a text vectorization layer"""

vectorize_layer = TextVectorization(max_tokens =max_tokens, output_sequence_length = max_sequence_len )

"""This will vectorize the filtered_reduced_data df and also will build the vocabulary automatically"""

vectorize_layer.adapt(data= np.array(filtered_reduced_data["text"]))

"""Uptil now our data has been adapted to vectorize layer, now we will actually convert the text tweets into integer form to be fed into neural net"""

def vectorize_tweet(raw_tweet):

  return vectorize_layer(raw_tweet).numpy()

vectorized_tweets = list(filtered_reduced_data["text"].apply(vectorize_tweet))

vectorized_tweets[0]

"""# Now we will divide our dataset into training and testing datasets"""

vectorized_train_tweets = vectorized_tweets[0:100000]
vectorized_cv_tweets = vectorized_tweets[100000:133000]

tweet_to_be_predicted = vectorized_tweets[0]
tweet_to_be_predicted = np.expand_dims(np.array(tweet_to_be_predicted), axis=0)

print(tweet_to_be_predicted)

"""now we will change the target values from 0 and 4 to 0 and 1"""

filtered_reduced_data["target"] = filtered_reduced_data["target"].apply(lambda x: str(x))

tweet_labels = list(filtered_reduced_data["target"].replace(to_replace = filtered_reduced_data["target"].unique(),
                                                            value = list(range(len(filtered_reduced_data["target"].unique())))))



"""now divide labels into trian labels and cv labels"""

train_labels = tweet_labels[0:100000]
cv_labels = tweet_labels[100000:133000]

"""# **Now we will create a data generator for training! its so interesting**"""

train_mb_size = 1000       #its 1000 in original code
num_epochs = 10
train_size = 100000         #its 200000 in original code

"""-Here the outside for loop runs for 10 times
-the inner for loop runs for 100 times
 and at each iteration of the inner loop both the lists are assigned hundred
 tweets and labels respectively

-np.expand will simply make the horizontal array into a vertical one
"""

def train_datagen():

  for _ in range(num_epochs):

    for i in range(train_size//train_mb_size):

      tweets_mb_list = [vectorized_train_tweets[arr_idx] for arr_idx in range(i*train_mb_size, (i+1)*train_mb_size)]
      tweets_labels_mb_list = [np.array(tweet_label) for tweet_label in train_labels[i*train_mb_size:(i+1)*train_mb_size]]

      yield np.array(tweets_mb_list), np.expand_dims(np.array(tweets_labels_mb_list),-1)



for tweets_mb, tweets_labels in train_datagen():

  print(tweets_mb.shape)
  print(tweets_labels.shape)

  break

cv_mb_size = 1000
cv_size = 33000

"""Same as train_datagen only the fact that this would be used for validation"""

def cv_datagen():

  for _ in range(num_epochs):

    for i in range(cv_size//cv_mb_size):

      tweets_mb_list = [vectorized_cv_tweets[arr_idx] for arr_idx in range(i*cv_mb_size,(i+1)*cv_mb_size)]
      tweets_labels_mb_list = [np.array(tweet_label) for tweet_label in cv_labels[i*cv_mb_size:(i+1)*cv_mb_size]]

      yield np.array(tweets_mb_list), np.expand_dims(np.array(tweets_labels_mb_list),-1)

for tweets_mb,tweets_labels in cv_datagen():

  print(tweets_mb.shape)
  print(tweets_labels.shape)

  break

!wget https://nlp.stanford.edu/data/glove.6B.zip

!unzip /content/glove.6B.zip

max_vocabulary_size = len(vectorize_layer.get_vocabulary())
embedding_output_dim = 50
print(max_vocabulary_size)

def create_bin_class_rnn():

  rnn_model = Sequential()

  rnn_model.add(Input(shape=(None,), dtype="int64"))
  rnn_model.add(Embedding(input_dim=max_vocabulary_size, output_dim= embedding_output_dim, input_length= max_sequence_len))
  rnn_model.add(LSTM(units=50))
  rnn_model.add(Dense(units=1,activation="sigmoid"))

  return rnn_model

rnn_model = create_bin_class_rnn()

rnn_model.summary()

rnn_model.compile(loss= "binary_crossentropy",metrics= ["accuracy",
                                                        tf.keras.metrics.Precision(),
                                                        tf.keras.metrics.Recall()])

training_data_gen = train_datagen()
cv_data_gen = cv_datagen()

rnn_model.fit(training_data_gen,
              epochs= num_epochs,
              validation_data= cv_data_gen,
              steps_per_epoch= 100, validation_steps= 10 )

result = rnn_model.predict(tweet_to_be_predicted)
print(result)

